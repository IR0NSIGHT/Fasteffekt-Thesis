% IV. Implementierung 	"technical details"

In diesem Kapitel beschreiben wir die Implementation von FastEffekt.
Dazu unterteilen wir das Tool in zwei Kategorien:
\begin{itemize}
    \item der Javascript Wrapper, welcher die Benchmarks ausführt
    \item die Effekt Benchmarks
\end{itemize}     
Wir betrachten jede Kategorie einzeln und beschreiben dabei Spezifikation von Input/Output, die technische Umsetzung, sowie die ausführende Umgebung und Entwicklungsumgebungen. Unterschiede zur Originalimplementation werden teilweise angesprochen, sind aber vertieft in 3. Ergebnisse diskutiert. %TODO link

\subsection{Javascript Command Line Tool}
Das User Interface von FastEffekt ist ein Javascript Command Line Tool.
Wir können das Tool über die Kommandozeile aufrufen und dabei spezifizieren welches Effekt Backend verwendet werden soll und ob eine minimale Ausführung stattfinden soll oder eine komplette:
\begin{lstlisting}[language=JavaScript]
max-priv@NOAH367-L:~$ FastEffekt -h

FastEffekt - by Maximilian Marschall
benchmarking the current install of the effekt language.
will execute benchmarks in effekt and JS.
outputs results to console and JSON file
execute all benchmarks: FastEffekt [--small] [backend]
run with: 
options:
    --help, -h:      documentation
    --small, -s:     run minimal benchmark to verify they all work
    --version, -v:   show version of FastEffekt
    --verbose        verbose logging
    backend:         which effekt backend to use. defaults to JS. passed        directly to effekt.sh
\end{lstlisting}

Wenn wir das Tool aufrufen, werden nacheinander die Effekt Benchmarks kompiliert und dann ausgeführt, in jeweils einem neuen Subprocess in der Kommandozeile pro Benchmark. Dadurch sind die Benchmarks räumlich und zeitlich voneinander so isoliert wie möglich und können nicht gegenseitig ihre Performance beeinflussen.
Dabei wird einmal die Effekt Version des Benchmarks ausgeführt und in einem weiteren Subprocess die JavaScript Version des Benchmarks. Beide laufen mehrfach hintereinander in ihrem jeweiligen Subprocess und geben die gemessene Zeit in Millisekunden zurück an das Kommandozeilen Werkzeug.

Ein Aufruf des Tools dauert auf unserem Entwicklerlaptop etwa eine Minute:
\begin{lstlisting}[language=JavaScript]
max-priv@NOAH367-L:~$ FastEffekt
run for backend js
running benchmark: permute
running benchmark: nbody
running benchmark: list
running benchmark: mandelbrot
running benchmark: bounce
running benchmark: towers
running benchmark: sieve
running benchmark: storage
running benchmark: queens
Mini analysis: [
  { name: 'permute', effekt: 1300, js: 6, ratio: 216.66666666666666 },
  { name: 'nbody', effekt: 27793, js: 251, ratio: 110.72908366533865 },
  { name: 'list', effekt: 208, js: 3, ratio: 69.33333333333333 },
  {
    name: 'mandelbrot',
    effekt: 82317,
    js: 1328,
    ratio: 61.98569277108434
  },
  { name: 'bounce', effekt: 74, js: 3, ratio: 24.666666666666668 },
  { name: 'towers', effekt: 5986, js: 70, ratio: 85.51428571428572 },
  { name: 'sieve', effekt: 5169, js: 44, ratio: 117.47727272727273 },
  { name: 'storage', effekt: 38, js: 7, ratio: 5.428571428571429 },
  { name: 'queens', effekt: 58, js: 5, ratio: 11.6 }
]
Verbose analysis saved to FastEffekt_results.json
\end{lstlisting}

Nachdem alle Benchmarks ausgeführt wurden, findet eine sehr rudimentäre erste Analyse der Laufzeiten statt, wie wir oben sehen können, die \textit{mini analysis}. Dabei wird der Durchschnitt der Laufzeiten eines Benchmarks berechnet für die Effekt Ausführung und für die JavaScript ausführung.
Die \textit{ratio} beschreibt das Verhältnis von Effekt zu JavaScript Laufzeitdurchschnitt und dient einer ersten Einschätzung der Performanz von Effekt.
Zusätzlich werden alle Datenpunkte abgespeichert in FastEffekt\_results.json.

\begin{lstlisting}[language=javascript]
max-priv@NOAH367-L:~$ cat FastEffekt_results.json 
[
    {
        "name": "nbody",
        "effekt": {
            "sum": 27793,
            "avg": 2779.3,
            "durations": [2858,2534,2433,3167,2803,2776,2827,2841,2765,2789]
        },
        "js": {
            "sum": 251,
            "avg": 25.1,
            "durations": [34,28,26,24,22,23,24,24,23,23]
        },
        "ratio": 110.72908366533865
    },
    {
        "name": "mandelbrot",
        "effekt": {
            "sum": 82317,
            "avg": 8231.7,
            "durations": [8963,9084,8063,8034,8415,7636,8097,7976,8005,8044]
        },
        "js": {
            "sum": 1328,
            "avg": 132.8,
            "durations": [119,350,99,97,96,97,112,118,115,125]
        },
        "ratio": 61.98569277108434
    },
    [...]
]
\end{lstlisting}
Jeder Eintrag im \textit{durations} Array ist eine Iteration des jeweiligen Benchmarks. Die Iterationen finden dabei im selben Subprocess und Aufruf des Benchmarks statt.

\subsubsection{Entwicklungsumgebung und Werkzeuge}
Um das FastEffekt Tool zu entwickeln und zu debuggen, haben wir Intellij Ultimate 2023.3.5 verwendet. Ultimate kommt von Haus aus mit weitreichender Unterstützung für Javascript.
Dazu zählen Syntax Highlighting, statische Analyse, Autoformatierung und ein Debugger/Stepper.

Entwickelt wurde auf der Plattform Ubuntu 22.04, verwaltet wird FastEffekt mit git und auf Github https://github.com/IR0NSIGHT/FastEffekt

Zur Verifikation während dem Entwicklungsprozess haben wir Github Actions verwendet, welche auf ubuntu-latest das Tool im verify-mode ausführt.

\subsubsection{Ausführende Umgebung}
Da FastEffekt ein NPM Projekt ist, braucht es eine NodeJS Umgebung.
Dafür verwenden wir node v18.16.1 auf einer ubuntu 22.04 Plattform.
Um das Tool systemweit zu installieren, wird NPM link verwendet, was einen Symlink auf das FastEffekt Repo im PATH hinterlegt.

In der Github Actions Pipeline wird die selbe Node Version verwendet.
Hier wollen wir anmerken, dass die Actions Pipeline nicht genutzt wird um Benchmark Zeitmessungen durchzuführen, da keine Garantien gegeben werden können welche (virtuelle) Maschine mit welcher Hardware die Benchmarks ausführt. Benchmark Laufzeiten wären nicht vergleichsfähig und dadurch nicht aussagekräftig.

\subsubsection{Spezifikation Input}
Wie wir oben bereits gesehen haben, hat FastEffekt einige Flags und Optionen:
\begin{lstlisting}
    options:
    --help, -h:      documentation
    --small, -s:     run minimal benchmark to verify they all work
    --version, -v:   show version of FastEffekt
    --verbose        verbose logging
    --backend=js     which effekt backend to use. defaults to JS. passed        directly to effekt.sh
\end{lstlisting}

Der --small Modus für FastEffekt ist für uns als Endnutzer eine Möglichkeit, einen minimal kurzen Durchlauf auszuführen. Dieser gibt keinen Aufschluss über die Performanzänderung der Benchmarks, sondern verifiziert nur, dass die Benchmarks kompilieren und laufen können. Dieser \textit{verify-mode} nutzt minimale Problemgrößen in den Benchmarks, um die Laufzeiten zu minimieren und überprüft, ob die Ergebnisse der Benchmarkberechnung für diese Problemgröße korrekt ist. Das bedeutet aber auch, dass keine Garantie gemacht wird, ob die Ergebnisse für andere Problemgrößen ebenfalls korrekt sind.

Die --verbose Flag existiert, damit wir als Endnutzer nachvollziehen können, wie FastEffekt die Benchmarks ausführt. Dabei logged das Tool in den Standard Output welche Kommandozeilenbefehle ausgeführt werden. So können wir schnell und einfach einzelne Schritte reproduzieren, falls ein Benchmark nicht kompiliert oder fehlschlägt:

\begin{lstlisting}
max-priv@NOAH367-L:~/Dokumente/repos/FastEffekt-Thesis$ FastEffekt -s --verbose
verify-mode: true
verbose
run for backend js
run all benchmarks with backend=js, small=--verify
running benchmark: permute
cd /home/max-priv/Dokumente/repos/FastEffekt/src/javascript/compare && cd ../../.. && ./out/permute 10 --verify
cd /home/max-priv/Dokumente/repos/FastEffekt/src/javascript/compare && cd ../../.. && node src/javascript/Permute.js 10 --verify
[...]
\end{lstlisting}

Mit der --backend=JS option, können wir spezifizieren, welches Effekt Backend verwendet werden soll. Wie bereits angesprochen in %TODO reference
Kapitel 2 Technologien, sind nicht alle Backends gleich weit fortgeschritten in der Entwicklung. Wir könnem als Endnutzer trotzdem alle existierenden Backends auswählen, das Tool wird uns informieren wenn Fehler bei Kompilierung oder Ausführung auftreten.

Im Moment sind Javascript und alle drei Chez Versionen verfügbar.
MLTon hat einen Bug mit Rekursion welcher zu Kompilerfehlern in allen Benchmarks führt und LLVM besitzt keine Implementation von mutable Arrays in der Standard library.

Ausgeführt wird immer nur ein Effekt Backend. Um verschiedene Backends bezüglich der Performance vergleich zu wollen, muss FastEffekt mehrfach ausgeführt werden.


\subsubsection{Fehler Handling}
Sollten bei einem Benchmark Fehler auftreten bei der Kompilierung oder beim Ausführen, werden diese aufgesammelt und am Ende in eine Logdatei geschrieben.
Der Benchmark wird nicht aufgeführt in der Ergebnisdatei.
Alle anderen Benchmarks laufen anschließend ganz normal weiter.
Dabei kann FastEffekt erkennen ob der Fehler bei der Kompilierung oder beim Ausführen des Benchmarks aufgetreten ist. Wir werden als Nutzer informiert über den Standardoutput und ein detaillierter Fehler kommt in die Fehlerlog Datei.

%FIXME: this error log points to the wrong file? thats for execution, not for compilation.
\begin{lstlisting}
max-priv@NOAH367-L:~/Dokumente/repos/FastEffekt$ cat FastEffekt_error.txt 
cd /home/max-priv/Dokumente/repos/FastEffekt/src/javascript/compare && cd ../../.. && ./out/nbody 10 --verify 
error:
compile error
status:
1
output:

[error] src/effekt/benchmark/nbody.effekt:192:9: Expected ;
      } else if (problemSize == 1) {
        ^
\end{lstlisting}
Hier finden wir dann welcher Befehl zu welchem Fehler mit welchen Details geführt hat.

\subsubsection{Implementationsdetails}
Wie bereits beschrieben ist FastEffekt ein NPM Projekt, maßgeblich damit wir mit möglichst wenig Aufwand das Tool systemweit zur Verfügung stellen können.
Libraries werden keine verwendet, lediglich einige Node Funktionalitäten um Subprocesses zu spawnen sowie im Dateisystem zu lesen und zu schreiben.

Der entrypoint ist src/javascript/index.js. Hier befindet sich die Logik für Kommandozeilenoperationen: Parsen von vom Nutzer definierten Parametern und das anschließende Aufrufen des Werkzeugs.
Die Logik für das Werkzeug befindet sich separat in einem eigenen Modul in src/javascript/compare/Comparator.js, hier werden wie oben beschrieben die Effekt Benchmarks in eigenen Subprocesses kompiliert, ausgeführt und die Daten analysiert und abgespeichert.

In der Originalimplementation von AWFY hat jede verwendete Sprache eine Harness Klasse, welche als zentraler entrypoint dient um Benchmarks auszuführen.
Unserer Einschätzung nach ist Effekt noch nicht weit genug entwickelt um damit ein Kommandozeilenwerkzeug zu entwickeln. Es fehlt mindestens Funktionalität für Dateioperationen und Prozessverwaltung, desweiteren besitzt Effekt keinen Debugger/Stepper, was die Entwicklung damit zusätzlich massiv verlangsamt. Wir benötigen ohnehin ein Wrapperprogramm um die Effekt Benchmarks herum, da wir eine begrenzte automatische Analyse der gemessenen Laufzeiten implementieren wollen. Weiterhin planen wir nicht die Effekt Benchmarks in das AWFY Projekt zu integrieren, sondern nur dessen Design der Benchmarks zu nutzen, nicht aber die aufrufende Struktur um die Benchmarks herum.

Aus diesen Gründen haben wir uns dazu entschieden, die Funktionalität für das Kompilieren, Ausführen und I/O zu extrahieren und nicht in Effekt, sondern in JavaScript zu implementieren. Diese bilden zusammen das Modul src/javascript/compare/comparator.

Dies stellt zwar eine Abweichung von AWFY dar und geht entgegen deren Spezifikation für die Implementation in neuen Sprachen. Allerdings ist dies nur eine technische Entscheidung, die die Performanz der Benchmarks nicht verändert und damit aus unserer Sicht keine relevante Abweichung darstellt.

\subsection{Effekt Benchmarks}
FastEffekt kompiliert Effekt Benchmarks und führt diese aus. Insgesamt existieren 9 Microbenchmarks in unserer Implementation, entwickelt aus der AWFY JavaScript Version der Benchmarks.
Jeder Benchmark ist ein unabhängiges Programm und dient als sein eigener EntryPoint, wir können also direkt einzelne Benchmarks über die Kommandozeile aufrufen:

\begin{lstlisting}[language=javascript]
max-priv@NOAH367-L:~/Dokumente/repos/FastEffekt$ effekt.sh -b src/effekt/benchmark/list.effekt 
max-priv@NOAH367-L:~/Dokumente/repos/FastEffekt$ ./out/list 10
[11,13,13,12,13,11,13,13,17,34]
\end{lstlisting}

Im ersten Schritt kompilieren wir den Benchmark in eine ausführbare Datei indem wir für Effekt die -b \textit{build flag} nutzen, sodass der kompilierte Benchmark als executable Datei im Ordner \./out hinterlegt wird. Im zweiten Schritt rufen wir die Executable auf. Da jeder Benchmark auch ein kleines Kommandozeilenwerkzeug ist, müssen wir die Anzahl Iterationen oder die --verify Flag angeben. --verify ist hier die Flag um den %TODO reference kaptile 
oben erwähnten verify-mode zu nutzen.
Wie wir sehen können, gibt uns der Aufruf der Executable einen JSON Array von Zahlen zurück. Jede Zahl beschreibt die Dauer eines Durchlaufs des Benchmarks in Millisekunden, jeder Benchmark durchläuft mehrere Iterationen. Die Anzahl der Iterationen wird von uns als Nutzer festgelegt.

\subsubsection{Entwicklungsumgebung und Werkzeuge}
Für die Entwicklung von Effektprogrammen gibt es in Visual Studio Code eine Erweiterung die Linter und Syntaxhighlighting Funktionalität bereitstellt:
https://github.com/effekt-lang/effekt-vscode
Zu diesem Zeitpunkt existiert kein Debugger/Stepper für Effekt, Consolelogging ist hier die nächstbeste Möglichkeit.

Wie in %TODO link
Kapitel oben beschrieben, führen wir zur Verifikation von Änderungen FastEffekt in einer Github Action aus, alle 9 Benchmarks werden dabei im verify-mode ausgeführt. 

\subsubsection{Ausführende Umgebung}
Zur Ausführung der Effekt Benchmarks brauchen wir eine Installation von Effekt selbst, die Version auf der wir FastEffekt entwickelt haben ist der Commit d8a1fba426511663c97386eedc00b244744fa6e4 im Effekt Repository.
Da Effekt nur selten neue Versionen released haben wir uns arbiträt für diese, damals relativ neue Version entschieden, welche einige von uns gewünschte Änderungen und Fixes besitzt, die der latest-release Version vom September 2023 fehlen.

Effekt ist wie beschrieben noch in der Entwicklungsphase und breaking changes sind eine Regelmäßigkeit auf dem Master Branch. Deshalb ist davon auszugehen dass FastEffekt ausschließlich auf dem genannten Commit funkioniert, es besteht definitiv keine Rückwärtskompatibliltät zu älteren Versionen.

Effekt benötigt für die Kompilierung und das Ausführen desweiteren Installationen der jeweiligen Backends, für uns relevant sind davon JavaScript, Chez-lift, Chez-callcc und Chez-Monadic.

Für JavaScript nutzen wir weiterhin node v18.16.1, für die Chez-Backends, welche Scheme nutzen, haben wir Chez Scheme Version 9.9.9-pre-release.23.

Der Vollständigkeit halber wollen wir hier erwähnen, dass MLton 20201002 genutzt wurde, auch wenn die Benchmarks nie lauffähig waren in diesem Backend.
Das selbe gilt für LLV %TODO which llvm version did i use?

In der Github Action nutzen wir ausschließlich das JavaScript Backend.

Die Plattform ist auch hier Ubuntu 22.04.

\subsubsection{Spezifikation Input und Output}
Um einen Benchmark auszuführen in Effekt, ohne das JavaScript Kommandozeilenwerkzeug zu nutzen, muss der Benchmark erst kompiliert werden. Es ist zwar grundsätzlich möglich in einem Schritt zu kompilieren und auszuführen in Effekt, allerdings ist dabei keine Übergabe von Kommandozeilenparametern möglich. Die geht nur, wenn erst kompiliert wird zu einer Executable mit der build-Flag, und im zweiten Schritt die Executable aufgerufen wird mit den gewünschten Kommandozeilenparametern.
Der Benchmark fordert hier entweder die Nutzung der --verify Flag oder eine Angabe der Anzahl Iterationen.
Da die Chez-Backends im Moment noch einen Bug aufweisen, wo keine Kommandozeilenparameter genutzt werden können, defaultet der Benchmark zu Iterationsanzahl = 3, für den Fall dass nichts angegeben wird.

Der Output des Programms ist ein JSON Array von Integers, welche jeweils die Dauer einer Iteration des Benchmarks in Millisekunden beschreiben, dabei ist der Return-Code des Prozesses 0.
Wenn ein Fehler auftritt ist der Return Code ungleich 0, der konkrete Output ist abhängig vom verwendeten Backend und vom Fehler. 

\subsubsection{Implementationsdetails}
Alle Microbenchmarks sind grundlegend gleich aufgebaut, hier stellen wir dies am Beispiel des \textit{Sieve} Benchmarks vor.
Es gibt eine \textit{benchmark(): Int} Funktion, welche die Benchmark Logik ausführt und eine Integer zurückgibt. Die Bedeutung der Integer ist Benchmark abhängig, in allen Benchmarks wird das Ergebnis verifiziert. Die Verifikation passiert in \textit{verifyResult(Int)}, dort sind für bestimmte Problemgrößen jeweils die erwarteten Ergebniswerte hinterlegt.
Die importierte Funktion \textit{innerBenchmarkLoop(n){ benchmark }{ verifyResult }} führt n Iterationen oft den Benchmark aus, verifiziert jedes Mal das Ergebnis und misst die Zeit für den Aufruf der Funktion.

Die Funktionen \textit{miniBench()} und \textit{normalBench()} führend beide den Benchmark aus, mit unterschiedlichen Problemgrößen. miniBench dient der Verifikation, die Problemgröße ist minimal um die Laufzeit zu minimieren.
normalBench ist für Performanzmessungen gedacht und nutzt normalerweise Problemgrößen > 1. In einigen Microbenchmarks ist die Problemgröße für beide Funktionen gleich, oder die Problemgröße synonym mit Anzahl innerer Iterationen.

\begin{lstlisting}[language=JavaScript]
def Sieve() = {
    def sieve(flags: Array[Boolean], size: Int): Int = {
        var primeCount: Int = 0;
        each(2, size){ i =>
        if (flags.unsafeGet(i - 1)) {
            primeCount = primeCount + 1;
            var k = i + i;
            while (k <= size) {
            put(flags, (k - 1), false);
            k = k + i;
            }
        }
        }
        return primeCount;
    }
    
    def benchmark() = {
        val flags: Array[Boolean] = fill[Boolean](5000, true);
        return sieve(flags, 5000);
        }
    
    def  verifyResult(result: Int) = {
        return 669 == result;
    }
    
    return innerBenchmarkLoop(1){benchmark}{verifyResult}   
}
    
def miniRun() = {
    Sieve()
}

def normalRun() = {
    each(0,150){ i =>
        Sieve()
    }
}

def main() = {
    println(runFromCli{ miniRun }{ normalRun })
}
\end{lstlisting}
Die main Funktion gibt die Benchmark-ausführenden Funktionen weiter an runFromCli. Hierbei handelt es sich um eine importierte Funktionalität, welche gegebene Kommandozeilenparameter einliest, parsed und dann mit n Iterationen normalRun ausführt, bei Abwesenheit der --verify Flag, oder 3 mal den miniRun.
Für jede Iteration wird die Zeit gemessen und anschließend alle Zeiten als JSON Array formatiert an main zurückgegeben. Von dort werden sie in den Standardoutput geprintet.
