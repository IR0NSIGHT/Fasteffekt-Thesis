% IV. Implementierung 	"technical details"

In diesem Kapitel beschreiben wir die Implementation von FastEffekt.
Dazu unterteilen wir das Tool in zwei Kategorien:
\begin{itemize}
    \item der Javascript Wrapper, welcher die Benchmarks ausführt
    \item die Effekt Benchmarks
\end{itemize}     
Wir betrachten jede Kategorie einzeln und beschreiben dabei Spezifikation von Input/Output, die technische Umsetzung und wie weit vom Original abgewichen wurde, sowie die ausführende Umgebung und Entwicklungsumgebungen.

\subsection{Javascript Command Line Tool}
Das User Interface von FastEffekt ist ein Javascript Command Line Tool.
Wir können das Tool über die Kommandozeile aufrufen und dabei spezifizieren welches Effekt Backend verwendet werden soll und ob eine minimale Ausführung stattfinden soll oder eine komplette:
\begin{lstlisting}[language=JavaScript]
max-priv@NOAH367-L:~$ FastEffekt -h

FastEffekt - by Maximilian Marschall
benchmarking the current install of the effekt language.
will execute benchmarks in effekt and JS.
outputs results to console and JSON file
execute all benchmarks: FastEffekt [--small] [backend]
run with: 
options:
    --help, -h:      documentation
    --small, -s:     run minimal benchmark to verify they all work
    --version, -v:   show version of FastEffekt
    --verbose        verbose logging
    backend:         which effekt backend to use. defaults to JS. passed        directly to effekt.sh
\end{lstlisting}

Wenn wir das Tool aufrufen, werden nacheinander die Effekt Benchmarks kompiliert und dann ausgeführt, in jeweils einem neuen Subprocess in der Kommandozeile pro Benchmark. Dadurch sind die Benchmarks räumlich und zeitlich voneinander so isoliert wie möglich und können nicht gegenseitig ihre Performance beeinflussen.
Dabei wird einmal die Effekt Version des Benchmarks ausgeführt und in einem weiteren Subprocess die JavaScript Version des Benchmarks. Beide laufen mehrfach hintereinander in ihrem jeweiligen Subprocess und geben die gemessene Zeit in Millisekunden zurück an das Kommandozeilen Werkzeug.

Ein Aufruf des Tools dauert auf unserem Entwicklerlaptop etwa eine Minute:
\begin{lstlisting}
max-priv@NOAH367-L:~$ FastEffekt
run for backend js
running benchmark: permute
running benchmark: nbody
running benchmark: list
running benchmark: mandelbrot
running benchmark: bounce
running benchmark: towers
running benchmark: sieve
running benchmark: storage
running benchmark: queens
Mini analysis: [
  { name: 'permute', effekt: 1300, js: 6, ratio: 216.66666666666666 },
  { name: 'nbody', effekt: 27793, js: 251, ratio: 110.72908366533865 },
  { name: 'list', effekt: 208, js: 3, ratio: 69.33333333333333 },
  {
    name: 'mandelbrot',
    effekt: 82317,
    js: 1328,
    ratio: 61.98569277108434
  },
  { name: 'bounce', effekt: 74, js: 3, ratio: 24.666666666666668 },
  { name: 'towers', effekt: 5986, js: 70, ratio: 85.51428571428572 },
  { name: 'sieve', effekt: 5169, js: 44, ratio: 117.47727272727273 },
  { name: 'storage', effekt: 38, js: 7, ratio: 5.428571428571429 },
  { name: 'queens', effekt: 58, js: 5, ratio: 11.6 }
]
Verbose analysis saved to FastEffekt_results.json
\end{lstlisting}

Nachdem alle Benchmarks ausgeführt wurden, findet eine sehr rudimentäre erste Analyse der Laufzeiten statt, wie wir oben sehen können, die \textit{mini analysis}. Dabei wird der Durchschnitt der Laufzeiten eines Benchmarks berechnet für die Effekt Ausführung und für die JavaScript ausführung.
Die \textit{ratio} beschreibt das Verhältnis von Effekt zu JavaScript Laufzeitdurchschnitt und dient einer ersten Einschätzung der Performanz von Effekt.
Zusätzlich werden alle Datenpunkte abgespeichert in FastEffekt\_results.json.

\begin{lstlisting}[language=javascript]
max-priv@NOAH367-L:~$ cat FastEffekt_results.json 
[
    {
        "name": "nbody",
        "effekt": {
            "sum": 27793,
            "avg": 2779.3,
            "durations": [2858,2534,2433,3167,2803,2776,2827,2841,2765,2789]
        },
        "js": {
            "sum": 251,
            "avg": 25.1,
            "durations": [34,28,26,24,22,23,24,24,23,23]
        },
        "ratio": 110.72908366533865
    },
    {
        "name": "mandelbrot",
        "effekt": {
            "sum": 82317,
            "avg": 8231.7,
            "durations": [8963,9084,8063,8034,8415,7636,8097,7976,8005,8044]
        },
        "js": {
            "sum": 1328,
            "avg": 132.8,
            "durations": [119,350,99,97,96,97,112,118,115,125]
        },
        "ratio": 61.98569277108434
    },
    [...]
]
\end{lstlisting}
Jeder Eintrag im \textit{durations} Array ist eine Iteration des jeweiligen Benchmarks. Die Iterationen finden dabei im selben Subprocess und Aufruf des Benchmarks statt.

\subsubsection{Entwicklungsumgebung und Werkzeuge}
Um das FastEffekt Tool zu entwickeln und zu debuggen, haben wir Intellij Ultimate 2023.3.5 verwendet. Ultimate kommt von Haus aus mit weitreichender Unterstützung für Javascript.
Dazu zählen Syntax Highlighting, statische Analyse, Autoformatierung und ein Debugger/Stepper.

Entwickelt wurde auf der Plattform Ubuntu 22.04, verwaltet wird FastEffekt mit git und auf Github https://github.com/IR0NSIGHT/FastEffekt

Zur Verifikation während dem Entwicklungsprozess haben wir Github Actions verwendet, welche auf ubuntu-latest das Tool im verify-mode ausführt.

\subsubsection{Ausführende Umgebung}
Da FastEffekt ein NPM Projekt ist, braucht es eine NodeJS Umgebung.
Dafür verwenden wir node v18.16.1 auf einer ubuntu 22.04 Plattform.
Um das Tool systemweit zu installieren, wird NPM link verwendet, was einen Symlink auf das FastEffekt Repo im PATH hinterlegt.

In der Github Actions Pipeline wird die selbe Node Version verwendet.
Hier wollen wir anmerken, dass die Actions Pipeline nicht genutzt wird um Benchmark Zeitmessungen durchzuführen, da keine Garantien gegeben werden können welche (virtuelle) Maschine mit welcher Hardware die Benchmarks ausführt. Benchmark Laufzeiten wären nicht vergleichsfähig und dadurch nicht aussagekräftig.

\subsubsection{Spezifikation Input}
Wie wir oben bereits gesehen haben, hat FastEffekt einige Flags und Optionen:
\begin{lstlisting}
    options:
    --help, -h:      documentation
    --small, -s:     run minimal benchmark to verify they all work
    --version, -v:   show version of FastEffekt
    --verbose        verbose logging
    --backend=js     which effekt backend to use. defaults to JS. passed        directly to effekt.sh
\end{lstlisting}

Der --small Modus für FastEffekt ist für uns als Endnutzer eine Möglichkeit, einen minimal kurzen Durchlauf auszuführen. Dieser gibt keinen Aufschluss über die Performanzänderung der Benchmarks, sondern verifiziert nur, dass die Benchmarks kompilieren und laufen können. Dieser \textit{verify-mode} nutzt minimale Problemgrößen in den Benchmarks, um die Laufzeiten zu minimieren und überprüft, ob die Ergebnisse der Benchmarkberechnung für diese Problemgröße korrekt ist. Das bedeutet aber auch, dass keine Garantie gemacht wird, ob die Ergebnisse für andere Problemgrößen ebenfalls korrekt sind.

Die --verbose Flag existiert, damit wir als Endnutzer nachvollziehen können, wie FastEffekt die Benchmarks ausführt. Dabei logged das Tool in den Standard Output welche Kommandozeilenbefehle ausgeführt werden. So können wir schnell und einfach einzelne Schritte reproduzieren, falls ein Benchmark nicht kompiliert oder fehlschlägt:

\begin{lstlisting}
max-priv@NOAH367-L:~/Dokumente/repos/FastEffekt-Thesis$ FastEffekt -s --verbose
verify-mode: true
verbose
run for backend js
run all benchmarks with backend=js, small=--verify
running benchmark: permute
cd /home/max-priv/Dokumente/repos/FastEffekt/src/javascript/compare && cd ../../.. && ./out/permute 10 --verify
cd /home/max-priv/Dokumente/repos/FastEffekt/src/javascript/compare && cd ../../.. && node src/javascript/Permute.js 10 --verify
[...]
\end{lstlisting}

Mit der --backend=JS option, können wir spezifizieren, welches Effekt Backend verwendet werden soll. Wie bereits angesprochen in %TODO reference
Kapitel 2 Technologien, sind nicht alle Backends gleich weit fortgeschritten in der Entwicklung. Wir könnem als Endnutzer trotzdem alle existierenden Backends auswählen, das Tool wird uns informieren wenn Fehler bei Kompilierung oder Ausführung auftreten.

Im Moment sind Javascript und alle drei Chez Versionen verfügbar.
MLTon hat einen Bug mit Rekursion welcher zu Kompilerfehlern in allen Benchmarks führt und LLVM besitzt keine Implementation von mutable Arrays in der Standard library.

Ausgeführt wird immer nur ein Effekt Backend. Um verschiedene Backends bezüglich der Performance vergleich zu wollen, muss FastEffekt mehrfach ausgeführt werden.


\subsubsection{Fehler Handling}
Sollten bei einem Benchmark Fehler auftreten bei der Kompilierung oder beim Ausführen, werden diese aufgesammelt und am Ende in eine Logdatei geschrieben.
Der Benchmark wird nicht aufgeführt in der Ergebnisdatei.
Alle anderen Benchmarks laufen anschließend ganz normal weiter.
Dabei kann FastEffekt erkennen ob der Fehler bei der Kompilierung oder beim Ausführen des Benchmarks aufgetreten ist. Wir werden als Nutzer informiert über den Standardoutput und ein detaillierter Fehler kommt in die Fehlerlog Datei.

%FIXME: this error log points to the wrong file? thats for execution, not for compilation.
\begin{lstlisting}
max-priv@NOAH367-L:~/Dokumente/repos/FastEffekt$ cat FastEffekt_error.txt 
cd /home/max-priv/Dokumente/repos/FastEffekt/src/javascript/compare && cd ../../.. && ./out/nbody 10 --verify 
error:
compile error
status:
1
output:

[error] src/effekt/benchmark/nbody.effekt:192:9: Expected ;
      } else if (problemSize == 1) {
        ^
\end{lstlisting}
Hier finden wir dann welcher Befehl zu welchem Fehler mit welchen Details geführt hat.

\subsubsection{Implementationsdetails}
Wie bereits beschrieben ist FastEffekt ist ein NPM Projekt, maßgeblich damit wir mit möglichst wenig Aufwand das Tool systemweit zur Verfügung stellen können.
Libraries werden keine verwendet, lediglich einige Node Funktionalitäten um Subprocesses zu spawnen und im Dateisystem zu lesen und zu schreiben.

Der entrypoint ist src/javascript/index.js. Hier befindet sich die Logik für Kommandozeilenoperationen: Parsen von vom Nutzer definierten Parametern und das anschließende Aufrufen des Werkzeugs.
Die Logik für das Werkzeug befindet sich separat in einem eigenen Modul in src/javascript/compare/Comparator.js, hier werden wie oben beschrieben die Effekt Benchmarks in eigenen Subprocesses kompiliert, ausgeführt und die Daten analysiert und abgespeichert.

In der Originalimplementation von AWFY hat jede verwendete Sprache eine Harness Klasse, welche als zentraler entrypoint dient um Benchmarks auszuführen.
Unserer Einschätzung nach ist Effekt noch nicht weit genug entwickelt um damit ein Kommandozeilenwerkzeug zu entwickeln. Es fehlt mindestens Funktionalität für Dateioperationen und Prozessverwaltung, desweiteren besitzt Effekt keinen Debugger/Stepper, was die Entwicklung damit zusätzlich massiv verlangsamt. Wir benötigen ohnehin ein Wrapperprogramm um die Effekt Benchmarks herum, da wir eine begrenzte automatische Analyse der gemessenen Laufzeiten implementieren wollen. Weiterhin planen wir nicht die Effekt Benchmarks in das AWFY Projekt zu integrieren, sondern nur dessen Design der Benchmarks zu nutzen, nicht aber die aufrufende Struktur um die Benchmarks herum.

Aus diesen Gründen haben wir uns dazu entschieden, die Funktionalität für das Kompilieren, Ausführen und I/O zu extrahieren und nicht in Effekt, sondern in JavaScript zu implementieren. Diese bilden zusammen das Modul src/javascript/compare/comparator.

Dies stellt zwar eine Abweichung von AWFY dar und geht entgegen deren Spezifikation für die Implementation in neuen Sprachen. Allerdings ist dies nur eine technische Entscheidung, die die Performanz der Benchmarks nicht verändert und damit aus unserer Sicht keine relevante Abweichung darstellt.

\subsection{Effekt Benchmarks}
FastEffekt kompiliert Effekt Benchmarks und führt diese aus. Insgesamt existieren 9 Microbenchmarks in unserer Implementation, entwickelt aus der AWFY JavaScript Version der Benchmarks.
Jeder Benchmark ist ein unabhängiges Programm und dient als sein eigener EntryPoint, wir können also direkt einzelne Benchmarks über die Kommandozeile aufrufen:
\begin{lstlisting}[language=javascript]
max-priv@NOAH367-L:~/Dokumente/repos/FastEffekt$ effekt.sh -b src/effekt/benchmark/list.effekt 
max-priv@NOAH367-L:~/Dokumente/repos/FastEffekt$ ./out/list 10
[11,13,13,12,13,11,13,13,17,34]
\end{lstlisting}
Im ersten Schritt kompilieren wir den Benchmark in eine ausführbare Datei indem wir für Effekt die -b \textit{build flag} nutzen, sodass der kompilierte Benchmark als executable Datei im Ordner \./out hinterlegt wird. Im zweiten Schritt rufen wir die Executable auf. Da jeder Benchmark auch ein kleines Kommandozeilenwerkzeug ist, müssen wir die Anzahl Iterationen oder die --verify Flag angeben. --verify ist hier die Flag um den %TODO reference kaptile 
oben erwähnten verify-mode zu nutzen.
Wie wir sehen können, gibt uns der Aufruf der Executable einen JSON Array von Zahlen zurück. Jede Zahl beschreibt die Dauer eines Durchlaufs des Benchmarks in Millisekunden, jeder Benchmark durchläuft mehrere Iterationen. Die Anzahl der Iterationen wird von uns als Nutzer festgelegt.

\subsubsection{Entwicklungsumgebung und Werkzeuge}
Für die Entwicklung von Effektprogrammen gibt es in Visual Studio Code eine Erweiterung die Linter und Syntaxhighlighting Funktionalität bereitstellt:
https://github.com/effekt-lang/effekt-vscode
Zu diesem Zeitpunkt existiert kein Debugger/Stepper für Effekt, Consolelogging ist hier die nächstbeste Möglichkeit.

Wie in %TODO link
Kapitel oben beschrieben, führen wir zur Verifikation von Änderungen FastEffekt in einer Github Action aus, alle 9 Benchmarks werden dabei im verify-mode ausgeführt. 

\subsubsection{Ausführende Umgebung}
Zur Ausführung der Effekt Benchmarks brauchen wir eine Installation von Effekt selbst, die Version auf der wir FastEffekt entwickelt haben ist der Commit d8a1fba426511663c97386eedc00b244744fa6e4 im Effekt Repository.
Da Effekt nur selten neue Versionen released haben wir uns arbiträt für diese, damals relativ neue Version entschieden, welche einige von uns gewünschte Änderungen und Fixes besitzt, die der latest-release Version vom September 2023 fehlen.

Effekt ist wie beschrieben noch in der Entwicklungsphase und breaking changes sind eine Regelmäßigkeit auf dem Master Branch. Deshalb ist davon auszugehen dass FastEffekt ausschließlich auf dem genannten Commit funkioniert, es besteht definitiv keine Rückwärtskompatibliltät zu älteren Versionen.

Effekt benötigt für die Kompilierung und das Ausführen desweiteren Installationen der jeweiligen Backends, für uns relevant sind davon JavaScript, Chez-lift, Chez-callcc und Chez-Monadic.

Für JavaScript nutzen wir weiterhin node v18.16.1, für die Chez-Backends, welche Scheme nutzen, haben wir Chez Scheme Version 9.9.9-pre-release.23.

Der Vollständigkeit halber wollen wir hier erwähnen, dass MLton 20201002 genutzt wurde, auch wenn die Benchmarks nie lauffähig waren in diesem Backend.
Das selbe gilt für LLV %TODO which llvm version did i use?

In der Github Action nutzen wir ausschließlich das JavaScript Backend.

Die Plattform ist auch hier Ubuntu 22.04.
\subsubsection{Spezifikation Input/Output}

\subsubsection{Implementationsdetails}
liste aller benchmarks mit besonderheiten und abweichung vom original benchmark



\subsection{Beschreibung der Implementierungsumgebung}

\subsubsection{ Microbenchmarks }
Implementiert in Effekt sind alle 9 Microbenchmarks aus Are-we-fast-yet:

Bounce simulates a ball bouncing within a box.

List recursively creates and traverses lists.

Mandelbrot calculates the classic fractal. It is derived from the Computer Languages Benchmark Game.

NBody simulates the movement of planets in the solar system. It is derived from the Computer Languages Benchmark Game.

Permute generates permutations of an array.

Queens solves the eight queens problem.

Sieve finds prime numbers based on the sieve of Eratosthenes.

Storage creates and verifies a tree of arrays to stress the garbage collector.

Towers solves the Towers of Hanoi game.

Die Implementation richtet sich dabei nach der Javascript Implementation aus Are-We-Fast-Yet.
Es gibt kleinere Abweichungen von der AWFY Implementation, hauptsächlich wie der Benchmark aufgerufen und ausgeführt wird. 
Jeder Benchmark ist eine eigenständige, ausführbare Effekt Datei. Er importiert CLIRunner und führt entweder den normal-mode oder mini-mode des Benchmarks aus. Minimode wird für Verifikation genutzt, Normal zur Zeitmessung.
Dies stellt eine Abweichung zur AWFY Implementation dar und ist durch das unfertige Import System von Effekt bedingt.
Durch diese etwas ungewöhnliche Inversion of Imports können ungewollte Nameclashes von Verschiedenen Benchmarks im CLI Runner verhindert werden, jeder Benchmark hat keine Imports nach aussen ausser den CLI Runner und manchmal SOM.effekt.
Der Benchmark misst die Zeit die jeder Durchlauf benötigt und gibt das Ergebnis als JSON-Array<Int> formatiert zurück in die Standardausgabe. Jede Zahl stellt dabei die Millisekunden dar, die ein Durchlauf dieses Benchmarks gebraucht hat.
Standardmäßig führt jeder Run den selben Benchmark mehrfach aus.
Gemessen wird dabei nur die Zeit die der Durchlauf zur Ausführung benötigt. Eventuelle Startupzeiten oder Laden von Abhängigkeiten werden nicht berücksichtigt.
Ausgeführt werden kann jeder Benchmark durch zwei Schritte: Executable compilen, Executable aufrufen mit Parametern.
Der Effekt Compiler kann zwar in einem Schritt direkt eine Datei compilen und ausführen, aber es ist dabei nicht möglich Kommandozeilen Argumente zu übergeben.
Stattdessen muss die build Flag genutzt werden:
effekt.sh -b meinBenchmark.effekt && ./out/meinBenchmark --verify 1

Hier sei am Rande erwähnt dass zwar die meisten aber nicht alle Backends so funktionieren. Für die Chez-Derivate Backends ist die Übergabe von Kommandozeilen argumenten durch einen Bug noch nicht möglich.

\subsubsection{ Runner }
\subsubsection{ CLI Tool }
  
Um bequem alle Benchmarks hintereinander auszuführen und die Ergebnisse zu loggen existiert das FastEffekt CLI Tool.
Es ist in Javascript geschrieben und ein NPM Projekt. Dadurch kann der Nutzer das Tool einfach herunterladen und in seinen PATH einbinden.
Ruft der Nutzer das Tool auf, lässt Comparator.js alle in der Konfiguration hinterlegten Benchmarks synchron kompilieren und dann ausführen, in jeweils eigenen Shell Threads. Compile und Runtime Fehler werden aufgefangen und in eine Fehler-logdatei geschrieben. Die Ergebnisse eines jeden Benchmarks aus dem standard output werden aufgefangen und zwischengespeichert.
Zusätzlich zu jedem Ausführen eines Benchmarks in Effekt, wird die Javascript Version des Benchmarks aus AWFY ausgeführt, gemessen und geloggt, analog zur Effekt Variante.
Alle Durchlauf Zeiten werden nach Benchmark name gelogged in eine Datei, formatiert als JSON.
Benchmarks, die fehlgeschlagen sind während compile oder runtime, tauchen nicht auf in der Output Datei.

Zur Übersichtlichkeit für den Nutzer werden nach Abschluss des Benchmarks die Durchschnittlichen Zeiten jedes Benchmarks in Effekt und Javascript angezeigt, sowie ein Vergleich Effekt/Javascript:

Mini analysis: [
{ name: 'permute', effekt: 1024, js: 5, ratio: 204.8 },
{ name: 'nbody', effekt: 24441, js: 217, ratio: 112.63133640552995 },
{ name: 'list', effekt: 103, js: 1, ratio: 103 },
{
name: 'mandelbrot',
effekt: 71809,
js: 905,
ratio: 79.34696132596684
},
{ name: 'bounce', effekt: 82, js: 4, ratio: 20.5 },
{ name: 'towers', effekt: 5117, js: 67, ratio: 76.3731343283582 },
{ name: 'sieve', effekt: 4699, js: 37, ratio: 127 },
{ name: 'storage', effekt: 25, js: 4, ratio: 6.25 },
{ name: 'queens', effekt: 38, js: 3, ratio: 12.666666666666666 }
] 


\subsubsection{ CI/CD Pipeline }

Zur Verifikation während des Entwicklungsprozesses wird Github Actions genutzt. In der Pipeline wird auf einer Ubuntu VM 
NPM und NodeJS installiert, sowie eine spezifische Version von Effekt. Dann wird FastEffekt im verify Modus ausgeführt. 
Damit werden alle Benchmarks mit Javascript als Backend ausgeführt mit einer minimalen Problemgröße, um zu verifizieren, dass die Implementationen noch lauffähig sind.
Die Pipeline schlägt fehl, wenn FastEffekt einen Errorcode zurück gibt.

\subsubsection{ Vergleich der Performance von verschiedenen Effekt Versionen }
 

\subsection{ Herausforderungen und Lösungsansätze während der Implementierung }

\subsubsection{ Import clashes }
Relativ früh im Entwicklungsprozess trat das Problem der Import Clashes auf. Wenn zwei Module in Effekt importiert werden,
die beide eine Funktion enthalten, welche den selben Namen hat, kommt es zum Clash. Es gibt für den Nutzer keine Möglichkeit 
auszuwählen, welche der beiden Funktionen benutzt wird. Es gibt auch keine Möglichkeit, nur einen Teil der Module zu importieren.
Importiert man ein Modul, wird immer alles importiert, analog zum sogenannten "Wildcard Import", einem aus Java bekannten Code Smell.

Da nicht auswählbar ist, welche der beiden Funktionen verwendet wird, schränkt es die Nutzung von imports massiv ein. Ein workaround ist, in einer
separaten Datei oder Modul einen der beiden Imports zu machen, dort alle Funktionen mit Wrappern neu zu definieren, und dann dieses neue Modul zu importieren.
Effektiv werden dabei alle genutzten Funktionalitäten mit Wrappern versehen um sie umzubennenen. Der Kosten Nutzen Aufwand ist dabei so hoch, dass es in FastEffekt nicht genutzt wird.

Als Alternative wird hier eine Inversion of Dependencies erzeugt: Es gibt nicht eine zentrale Klasse zu haben, welche alle Benchmarks importiert und ausführen kann.
Stattdessen ist jeder Benchmark selbst ein ausführbares Modul mit eigenem Entry Point. Die nötige Funktionalität um Kommandozeilen Argumente zu parsen und Zeit zu messen wird vom
CLI Runner Modul zur Verfügung gestellt, so hat jeder Benchmark nur die Abhängigkeit zum CLI Runner, statt der CLI Runner eine Abhängigkeit zu allen Benchmarks.

Der offensichtliche Vorteil ist, dass die Anzahl von gesamten Imports in ein Modul hinein minimal wird. Zusätzlich kann garantiert werden, dass ein Benchmark nicht von einem anderen Benchmarkmodul beeinflusst wird, 
wie beispielsweise durch erhöhte Ladezeiten oder RAM Bedarf. 
Allerdings entsteht daraus der Nachteil, dass das Programm keinen zentralen Entry Point mehr hat, und auf neun verschiedene Datein verteilt ist. Dementsprechend wird ein 
Wrapper Programm benötigt, sodass der Nutzer nur FastEffekt aufruft, und intern die einzelnen relevanten Benchmarks ausgeführt werden.

Aufgrund des experimentellen und oft ändernden Zustands, sowie teils fehlende Funktionalität von Effekt ist das Wrapper Programm nicht in Effekt implementiert, sondern in Javascript.

\subsubsection{unfertige backends}

\subsubsection{buggy backends}



\subsubsection{ Quality of Life }
Effekt ist eine funktionale Sprache. Are We Fast Yet wurde ursprünglich in Java implementiert, die als Vorlage ausgewählten Javascript Benchmarks nutzen auch stark objektorienterte Methoden.
So ist jeder Benchmark eine Klasse, welche vom Benchmark Interface erbt, ganz analog zur Java Implementation.
Diese Vorgehensweise ist in Effekt nicht möglich, Klassen und Objekte sind nicht implementiert, werden nicht unterstützt.

Interfaces sind möglich in Effekt, aber deutlich restriktiver: Es dürfen nur exakt die Member implementiert sein, welche vom Interface vorgegeben werden, nicht mehr und nicht weniger.
In Java und Javascript gibt es nur die Vorgabe, dass es mindestens die Member sein müssen, mehr sind aber erlaubt.

[...] //TODO


Auch Funktionsobjekte, welche in der allgemeinen Javascript Entwicklung viel genutzt werden, sind nicht möglich: Funktionen können nicht in Variablen "gespeichert" werden, können auch nicht von Funktionen returned werden.
Der einzige erlaubte Weg eine Funktion weiter zu geben, ist als Parameter einer Higher Order Funktion, tiefer in den Call Stack hinein.




\subsubsection{ Overload des Aktionspunktes }
Zwar sind aufrufe wie "meinInterface.machEtwas()" möglich, allerdings ist dies keine echtes Objekt Orientiertes paradigma, sondern nur Syntaktik Sugar.
Auch ist dieser Overload des "Aktionspunktes" so instabil, dass er kaum genutzt wird in FastEffekt. Eigentlich kann jede Funktion so aufgerufen werden, dass sie als Methode des ersten Parameters erscheint, 
doch der Compiler erkennt dies oft nicht an. Ein Großteil der Versuche dies zu benutzen führte zu Compiler Fehlermn, weshalb ich schnell davon Abstand genommen habe.

\subsubsection{ Zu schnelle Benchmarks }
Als Kompromiss zwischen Aussagekraft und kurzer Laufzeit des Tools wurde arbiträr eine Laufzeit von zwei Sekunden pro Benchmark in Effekt gewählt.
Mit zehn Microbenchmarks dauert ein kompletter Durchlauf von FastEffekt damit mindestens zwanzig Sekunden plus die Laufzeit der Javascript-AWFY-Vergleichsbenchmarks.
Gesteuert werden kann die Dauer eines Benchmark-Durchlaufs über die Problemgröße und über innere Iterationen. Ist ein Benchmark deutlich zu langsam, kann die Problemgröße reduziert werden, ist ein Benchmark deutlich zu schnell, kann die Anzahl innerer Iterationen künstlich erhöht werden. Damit bietet sich eine begrenzte Kontrolle über die Laufzeit an.

Wie bereits erwähnt, bilden die Javascript AWFY-Implementationen die Basis mit der die Performance der Effekt Benchmarks verglichen wird.
Da vorallem das Effekt-Javascript-Backend teils deutlich langsamer ist als die AWFY-Javascript Version, //TODO auf analyse section beziehen
 muss die Problemgröße der Benchmarks klein gehalten werden. Ansonsten überschreiten die Effekt Benchmarks die Zwei-Sekunden-Grenze, und benötigen bis zu 30 Sekunden. Wenn allerdings die Problemgröße gedrückt wird, fällt auch die Dauer der Javascript Benchmarks, bis unter 2 Millisekunden. Die Zeitauflösung der Benchmarks ist nicht feingranular genug, um unter 1 Millisekunde zu messen. Das führt zu geloggten Zeiten von 0,1 und 2 Millisekunden, für die meisten Benchmarks. 

Mit solchen grob granularen Zahlen lässt sich in der Analyse wenig anfangen. Mit einer Laufzeit von 0 Millisekunden wird sogar der Vergleich zur Effekt Laufzeit verfälscht und eine Ratio von positiv unendlich angegeben, oder Not a Number (NaN).

Eine Lösung wäre eine feinere Zeiterfassung in Javascript, Millis und Mikrosekunden. Allerdings ist hier zu beachten, dass möglicherweise trotz feinerer Messung wieder Schwankungen im Interpreter, Garbage Collection und Laden von Dependencies das Ergebnis stark beeinflussen können.

Eine alternative Lösung ist die Erhöhung der Problemgröße oder der inneren Iterationen, nur für Javascript, nicht aber für die Effekt Benchmarks.
Das hat den Vorteil dass die Javascript Laufzeit aussagekräftiger wird, aber die Vergleichbarkeit mit der Effekt Laufzeit sinkt.

Die dritte Lösung ist das Erhöhen von Problemgröße und inneren Iterationen in Effekt und Javascript, so bleibt Aussagekraft und Vergleichbarkeit erhalten.
Damit wird das FastEffekt Tool schmerzhaft langsam für den Endnutzer, wenn Laufzeiten von einer Minute und mehr erreicht werden, und verletzt damit das Kernrequirement eine schnelle Möglichkeit von Performance Messung zu bieten.

\subsubsection{ import clashes }
import clashes erzwingen inversion-of-imports (use cli-runner, single executables)

\subsection{Architektur des Programms}
\subsubsection{ CLI Tool mit NPM, JS }
\subsubsection{ Are We Fast Yet javascript und java }
\subsubsection{ ein runner lässt einen benchmark laufen und misst die zeit }
\subsubsection{ compile und init overhead reduzieren soweit möglich }
  
\subsection{ Performanz- und Funktionalitätstests }
\subsubsection{ baseline vergleich (vllt zu Future Work?) }
\subsubsection{ x iterationen eines benchmarks -> x laufzeiten  }
\subsubsection{ analysiere liste der laufzeiten jedes benchmarks }
    
\subsection{ Designentscheidungen für die Umsetzung in der neuen Programmiersprache }
\subsubsection{ ursprünglich: analog zu JS Benchmarks aus AWFY }
\subsubsection{ Designphilosophie ändert sich im Verlauf der Implementierung  }
\subsubsection{ Laufzeitumgebung/Referenz ist immer die eigene Maschine }
\subsubsection{CI/CD dient zur Verifikation, nicht zum Benchmarken  }
    {\rightarrow} keine Garantie über Einfluss der Maschine in der Pipeline auf Performance 
    