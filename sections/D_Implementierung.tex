% IV. Implementierung 	"technical details"

\subsection{Beschreibung der Implementierungsumgebung}

\subsubsection{VSC mit effekt linter}

Für die Implementation der Benchmarks in Effekt wird Visual Studio Code als IDE verwendet.
Effekt stellt dafür eine VSC Erweiterung zur Verfügung (https://github.com/effekt-lang/effekt-vscode), welches als Linter und Typechecker fungiert. 

\subsubsection{Intellij Ultimate für Javascript Debugging des runners}
Um das fasteffekt Tool selbst zu entwickeln und zu debuggen, wurde Intellij Ultimate verwendet. Ultimate kommt von Haus aus mit Javascript Unterstützung.

\subsubsection{github actions für CI/CD Pipeline}
Zur Verifikation von Änderungen während des Entwicklungsprozesses, wird Github Actions genutzt um fasteffekt im verify modus auszuführen. //FIXME use aktiv
 Dadurch kann sichergestellt werden, dass Änderungen an den Benchmarks nicht zum Fehlschlagen der Benchmarks während der Ausführung führt. Die Actions Pipeline wird nicht genutzt um Benchmark Timing durchzuführen, da keine Garantien gegeben werden können welche Maschine mit welcher Hardware die Benchmarks ausführt. Benchmark Timings wären nicht vergleichsfähig und dadurch nicht aussagekräftig.
    
\subsection{ Umsetzung des Benchmarking Tools } 
\subsubsection{ Microbenchmarks }
Implementiert in Effekt sind alle 9 Microbenchmarks aus Are-we-fast-yet:

Bounce simulates a ball bouncing within a box.

List recursively creates and traverses lists.

Mandelbrot calculates the classic fractal. It is derived from the Computer Languages Benchmark Game.

NBody simulates the movement of planets in the solar system. It is derived from the Computer Languages Benchmark Game.

Permute generates permutations of an array.

Queens solves the eight queens problem.

Sieve finds prime numbers based on the sieve of Eratosthenes.

Storage creates and verifies a tree of arrays to stress the garbage collector.

Towers solves the Towers of Hanoi game.

Die Implementation richtet sich dabei nach der Javascript Implementation aus Are-We-Fast-Yet.
Es gibt kleinere Abweichungen von der AWFY Implementation, hauptsächlich wie der Benchmark aufgerufen und ausgeführt wird. 
Jeder Benchmark ist eine eigenständige, ausführbare Effekt Datei. Er importiert CLIRunner und führt entweder den normal-mode oder mini-mode des Benchmarks aus. Minimode wird für Verifikation genutzt, Normal zur Zeitmessung.
Dies stellt eine Abweichung zur AWFY Implementation dar und ist durch das unfertige Import System von Effekt bedingt.
Durch diese etwas ungewöhnliche Inversion of Imports können ungewollte Nameclashes von Verschiedenen Benchmarks im CLI Runner verhindert werden, jeder Benchmark hat keine Imports nach aussen ausser den CLI Runner und manchmal SOM.effekt.
Der Benchmark misst die Zeit die jeder Durchlauf benötigt und gibt das Ergebnis als JSON-Array<Int> formatiert zurück in die Standardausgabe. Jede Zahl stellt dabei die Millisekunden dar, die ein Durchlauf dieses Benchmarks gebraucht hat.
Standardmäßig führt jeder Run den selben Benchmark mehrfach aus.
Gemessen wird dabei nur die Zeit die der Durchlauf zur Ausführung benötigt. Eventuelle Startupzeiten oder Laden von Abhängigkeiten werden nicht berücksichtigt.
Ausgeführt werden kann jeder Benchmark durch zwei Schritte: Executable compilen, Executable aufrufen mit Parametern.
Der Effekt Compiler kann zwar in einem Schritt direkt eine Datei compilen und ausführen, aber es ist dabei nicht möglich Kommandozeilen Argumente zu übergeben.
Stattdessen muss die build Flag genutzt werden:
effekt.sh -b meinBenchmark.effekt && ./out/meinBenchmark --verify 1

Hier sei am Rande erwähnt dass zwar die meisten aber nicht alle Backends so funktionieren. Für die Chez-Derivate Backends ist die Übergabe von Kommandozeilen argumenten durch einen Bug noch nicht möglich.

\subsubsection{ Runner }
\subsubsection{ CLI Tool }
  
Um bequem alle Benchmarks hintereinander auszuführen und die Ergebnisse zu loggen existiert das Fasteffekt CLI Tool.
Es ist in Javascript geschrieben und ein NPM Projekt. Dadurch kann der Nutzer das Tool einfach herunterladen und in seinen PATH einbinden.
Ruft der Nutzer das Tool auf, lässt Comparator.js alle in der Konfiguration hinterlegten Benchmarks synchron kompilieren und dann ausführen, in jeweils eigenen Shell Threads. Compile und Runtime Fehler werden aufgefangen und in eine Fehler-logdatei geschrieben. Die Ergebnisse eines jeden Benchmarks aus dem standard output werden aufgefangen und zwischengespeichert.
Zusätzlich zu jedem Ausführen eines Benchmarks in Effekt, wird die Javascript Version des Benchmarks aus AWFY ausgeführt, gemessen und geloggt, analog zur Effekt Variante.
Alle Durchlauf Zeiten werden nach Benchmark name gelogged in eine Datei, formatiert als JSON.
Benchmarks, die fehlgeschlagen sind während compile oder runtime, tauchen nicht auf in der Output Datei.

Zur Übersichtlichkeit für den Nutzer werden nach Abschluss des Benchmarks die Durchschnittlichen Zeiten jedes Benchmarks in Effekt und Javascript angezeigt, sowie ein Vergleich Effekt/Javascript:

Mini analysis: [
{ name: 'permute', effekt: 1024, js: 5, ratio: 204.8 },
{ name: 'nbody', effekt: 24441, js: 217, ratio: 112.63133640552995 },
{ name: 'list', effekt: 103, js: 1, ratio: 103 },
{
name: 'mandelbrot',
effekt: 71809,
js: 905,
ratio: 79.34696132596684
},
{ name: 'bounce', effekt: 82, js: 4, ratio: 20.5 },
{ name: 'towers', effekt: 5117, js: 67, ratio: 76.3731343283582 },
{ name: 'sieve', effekt: 4699, js: 37, ratio: 127 },
{ name: 'storage', effekt: 25, js: 4, ratio: 6.25 },
{ name: 'queens', effekt: 38, js: 3, ratio: 12.666666666666666 }
] 

Der Nutzer kann angeben welches Effekt Backend genutzt werden soll. Im Moment sind Javascript und alle drei Chez Versionen verfügbar.
MLTon hat einen Bug mit Rekursion, und LLVM besitzt keine Implementation von mutable Arrays in der Standard library.

Ausgeführt wird immer nur ein Effekt Backend. Um verschiedene Backends bezüglich der Performance vergleich zu wollen, muss fasteffekt mehrfach ausgeführt werden.  

\subsubsection{ CI/CD Pipeline }

Zur Verifikation während des Entwicklungsprozesses wird Github Actions genutzt. In der Pipeline wird auf einer Ubuntu VM 
NPM und NodeJS installiert, sowie eine spezifische Version von Effekt. Dann wird FastEffekt im verify Modus ausgeführt. 
Damit werden alle Benchmarks mit Javascript als Backend ausgeführt mit einer minimalen Problemgröße, um zu verifizieren, dass die Implementationen noch lauffähig sind.
Die Pipeline schlägt fehl, wenn FastEffekt einen Errorcode zurück gibt.

\subsubsection{ Vergleich der Performance von verschiedenen Effekt Versionen }
 

\subsection{ Herausforderungen und Lösungsansätze während der Implementierung }

\subsubsection{ Import clashes }
Relativ früh im Entwicklungsprozess trat das Problem der Import Clashes auf. Wenn zwei Module in Effekt importiert werden,
die beide eine Funktion enthalten, welche den selben Namen hat, kommt es zum Clash. Es gibt für den Nutzer keine Möglichkeit 
auszuwählen, welche der beiden Funktionen benutzt wird. Es gibt auch keine Möglichkeit, nur einen Teil der Module zu importieren.
Importiert man ein Modul, wird immer alles importiert, analog zum sogenannten "Wildcard Import", einem aus Java bekannten Code Smell.

Da nicht auswählbar ist, welche der beiden Funktionen verwendet wird, schränkt es die Nutzung von imports massiv ein. Ein workaround ist, in einer
separaten Datei oder Modul einen der beiden Imports zu machen, dort alle Funktionen mit Wrappern neu zu definieren, und dann dieses neue Modul zu importieren.
Effektiv werden dabei alle genutzten Funktionalitäten mit Wrappern versehen um sie umzubennenen. Der Kosten Nutzen Aufwand ist dabei so hoch, dass es in FastEffekt nicht genutzt wird.

Als Alternative wird hier eine Inversion of Dependencies erzeugt: Es gibt nicht eine zentrale Klasse zu haben, welche alle Benchmarks importiert und ausführen kann.
Stattdessen ist jeder Benchmark selbst ein ausführbares Modul mit eigenem Entry Point. Die nötige Funktionalität um Kommandozeilen Argumente zu parsen und Zeit zu messen wird vom
CLI Runner Modul zur Verfügung gestellt, so hat jeder Benchmark nur die Abhängigkeit zum CLI Runner, statt der CLI Runner eine Abhängigkeit zu allen Benchmarks.

Der offensichtliche Vorteil ist, dass die Anzahl von gesamten Imports in ein Modul hinein minimal wird. Zusätzlich kann garantiert werden, dass ein Benchmark nicht von einem anderen Benchmarkmodul beeinflusst wird, 
wie beispielsweise durch erhöhte Ladezeiten oder RAM Bedarf. 
Allerdings entsteht daraus der Nachteil, dass das Programm keinen zentralen Entry Point mehr hat, und auf neun verschiedene Datein verteilt ist. Dementsprechend wird ein 
Wrapper Programm benötigt, sodass der Nutzer nur FastEffekt aufruft, und intern die einzelnen relevanten Benchmarks ausgeführt werden.

Aufgrund des experimentellen und oft ändernden Zustands, sowie teils fehlende Funktionalität von Effekt ist das Wrapper Programm nicht in Effekt implementiert, sondern in Javascript.

\subsubsection{unfertige backends}

\subsubsection{buggy backends}



\subsubsection{ Quality of Life }
Effekt ist eine funktionale Sprache. Are We Fast Yet wurde ursprünglich in Java implementiert, die als Vorlage ausgewählten Javascript Benchmarks nutzen auch stark objektorienterte Methoden.
So ist jeder Benchmark eine Klasse, welche vom Benchmark Interface erbt, ganz analog zur Java Implementation.
Diese Vorgehensweise ist in Effekt nicht möglich, Klassen und Objekte sind nicht implementiert, werden nicht unterstützt.

Interfaces sind möglich in Effekt, aber deutlich restriktiver: Es dürfen nur exakt die Member implementiert sein, welche vom Interface vorgegeben werden, nicht mehr und nicht weniger.
In Java und Javascript gibt es nur die Vorgabe, dass es mindestens die Member sein müssen, mehr sind aber erlaubt.

[...] //TODO


Auch Funktionsobjekte, welche in der allgemeinen Javascript Entwicklung viel genutzt werden, sind nicht möglich: Funktionen können nicht in Variablen "gespeichert" werden, können auch nicht von Funktionen returned werden.
Der einzige erlaubte Weg eine Funktion weiter zu geben, ist als Parameter einer Higher Order Funktion, tiefer in den Call Stack hinein.




\subsubsection{ Overload des Aktionspunktes }
Zwar sind aufrufe wie "meinInterface.machEtwas()" möglich, allerdings ist dies keine echtes Objekt Orientiertes paradigma, sondern nur Syntaktik Sugar.
Auch ist dieser Overload des "Aktionspunktes" so instabil, dass er kaum genutzt wird in FastEffekt. Eigentlich kann jede Funktion so aufgerufen werden, dass sie als Methode des ersten Parameters erscheint, 
doch der Compiler erkennt dies oft nicht an. Ein Großteil der Versuche dies zu benutzen führte zu Compiler Fehlermn, weshalb ich schnell davon Abstand genommen habe.

\subsubsection{ Zu schnelle Benchmarks }
Als Kompromiss zwischen Aussagekraft und kurzer Laufzeit des Tools wurde arbiträr eine Laufzeit von zwei Sekunden pro Benchmark in Effekt gewählt.
Mit zehn Microbenchmarks dauert ein kompletter Durchlauf von FastEffekt damit mindestens zwanzig Sekunden plus die Laufzeit der Javascript-AWFY-Vergleichsbenchmarks.
Gesteuert werden kann die Dauer eines Benchmark-Durchlaufs über die Problemgröße und über innere Iterationen. Ist ein Benchmark deutlich zu langsam, kann die Problemgröße reduziert werden, ist ein Benchmark deutlich zu schnell, kann die Anzahl innerer Iterationen künstlich erhöht werden. Damit bietet sich eine begrenzte Kontrolle über die Laufzeit an.

Wie bereits erwähnt, bilden die Javascript AWFY-Implementationen die Basis mit der die Performance der Effekt Benchmarks verglichen wird.
Da vorallem das Effekt-Javascript-Backend teils deutlich langsamer ist als die AWFY-Javascript Version, //TODO auf analyse section beziehen
 muss die Problemgröße der Benchmarks klein gehalten werden. Ansonsten überschreiten die Effekt Benchmarks die Zwei-Sekunden-Grenze, und benötigen bis zu 30 Sekunden. Wenn allerdings die Problemgröße gedrückt wird, fällt auch die Dauer der Javascript Benchmarks, bis unter 2 Millisekunden. Die Zeitauflösung der Benchmarks ist nicht feingranular genug, um unter 1 Millisekunde zu messen. Das führt zu geloggten Zeiten von 0,1 und 2 Millisekunden, für die meisten Benchmarks. 

Mit solchen grob granularen Zahlen lässt sich in der Analyse wenig anfangen. Mit einer Laufzeit von 0 Millisekunden wird sogar der Vergleich zur Effekt Laufzeit verfälscht und eine Ratio von positiv unendlich angegeben, oder Not a Number (NaN).

Eine Lösung wäre eine feinere Zeiterfassung in Javascript, Millis und Mikrosekunden. Allerdings ist hier zu beachten, dass möglicherweise trotz feinerer Messung wieder Schwankungen im Interpreter, Garbage Collection und Laden von Dependencies das Ergebnis stark beeinflussen können.

Eine alternative Lösung ist die Erhöhung der Problemgröße oder der inneren Iterationen, nur für Javascript, nicht aber für die Effekt Benchmarks.
Das hat den Vorteil dass die Javascript Laufzeit aussagekräftiger wird, aber die Vergleichbarkeit mit der Effekt Laufzeit sinkt.

Die dritte Lösung ist das Erhöhen von Problemgröße und inneren Iterationen in Effekt und Javascript, so bleibt Aussagekraft und Vergleichbarkeit erhalten.
Damit wird das Fasteffekt Tool schmerzhaft langsam für den Endnutzer, wenn Laufzeiten von einer Minute und mehr erreicht werden, und verletzt damit das Kernrequirement eine schnelle Möglichkeit von Performance Messung zu bieten.

\subsubsection{ import clashes }
import clashes erzwingen inversion-of-imports (use cli-runner, single executables)

\subsection{Architektur des Programms}
\subsubsection{ CLI Tool mit NPM, JS }
\subsubsection{ Are We Fast Yet javascript und java }
\subsubsection{ ein runner lässt einen benchmark laufen und misst die zeit }
\subsubsection{ compile und init overhead reduzieren soweit möglich }
  
\subsection{ Performanz- und Funktionalitätstests }
\subsubsection{ baseline vergleich (vllt zu Future Work?) }
\subsubsection{ x iterationen eines benchmarks -> x laufzeiten  }
\subsubsection{ analysiere liste der laufzeiten jedes benchmarks }
    
\subsection{ Designentscheidungen für die Umsetzung in der neuen Programmiersprache }
\subsubsection{ ursprünglich: analog zu JS Benchmarks aus AWFY }
\subsubsection{ Designphilosophie ändert sich im Verlauf der Implementierung  }
\subsubsection{ Laufzeitumgebung/Referenz ist immer die eigene Maschine }
\subsubsection{CI/CD dient zur Verifikation, nicht zum Benchmarken  }
    {\rightarrow} keine Garantie über Einfluss der Maschine in der Pipeline auf Performance 
    